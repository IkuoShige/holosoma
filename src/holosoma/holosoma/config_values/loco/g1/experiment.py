from dataclasses import replace

from holosoma.config_types.algo import FPOModuleDictConfig, LayerConfig, ModuleConfig
from holosoma.config_types.experiment import ExperimentConfig, NightlyConfig, TrainingConfig
from holosoma.config_values import (
    action,
    algo,
    command,
    curriculum,
    observation,
    randomization,
    reward,
    robot,
    simulator,
    termination,
    terrain,
)

g1_29dof = ExperimentConfig(
    env_class="holosoma.envs.locomotion.locomotion_manager.LeggedRobotLocomotionManager",
    training=TrainingConfig(project="hv-g1-manager", name="g1_29dof_manager"),
    algo=replace(algo.ppo, config=replace(algo.ppo.config, num_learning_iterations=25000, use_symmetry=True)),
    simulator=simulator.isaacgym,
    robot=robot.g1_29dof,
    terrain=terrain.terrain_locomotion_mix,
    observation=observation.g1_29dof_loco_single_wolinvel,
    action=action.g1_29dof_joint_pos,
    termination=termination.g1_29dof_termination,
    randomization=randomization.g1_29dof_randomization,
    command=command.g1_29dof_command,
    curriculum=curriculum.g1_29dof_curriculum,
    reward=reward.g1_29dof_loco,
    nightly=NightlyConfig(
        iterations=5000,
        metrics={"Episode/rew_tracking_ang_vel": [0.7, "inf"], "Episode/rew_tracking_lin_vel": [0.55, "inf"]},
    ),
)

g1_29dof_fast_sac = ExperimentConfig(
    env_class="holosoma.envs.locomotion.locomotion_manager.LeggedRobotLocomotionManager",
    training=TrainingConfig(project="hv-g1-manager", name="g1_29dof_fast_sac_manager"),
    algo=replace(algo.fast_sac, config=replace(algo.fast_sac.config, num_learning_iterations=50000, use_symmetry=True)),
    simulator=simulator.isaacgym,
    robot=robot.g1_29dof,
    terrain=terrain.terrain_locomotion_mix,
    observation=observation.g1_29dof_loco_single_wolinvel,
    action=action.g1_29dof_joint_pos,
    termination=termination.g1_29dof_termination,
    randomization=randomization.g1_29dof_randomization,
    command=command.g1_29dof_command,
    curriculum=curriculum.g1_29dof_curriculum_fast_sac,
    reward=reward.g1_29dof_loco_fast_sac,
    nightly=NightlyConfig(
        iterations=50000,
        metrics={"Episode/rew_tracking_ang_vel": [0.8, "inf"], "Episode/rew_tracking_lin_vel": [0.95, "inf"]},
    ),
)

g1_29dof_fpo = ExperimentConfig(
    env_class="holosoma.envs.locomotion.locomotion_manager.LeggedRobotLocomotionManager",
    training=TrainingConfig(project="hv-g1-manager", name="g1_29dof_fpo_manager"),
    algo=replace(
        algo.fpo,
        config=replace(algo.fpo.config, num_learning_iterations=25000, use_symmetry=True, clip_param=0.01, lam=0.5),
    ),
    simulator=simulator.isaacgym,
    robot=robot.g1_29dof,
    terrain=terrain.terrain_locomotion_mix,
    observation=observation.g1_29dof_loco_single_wolinvel,
    action=action.g1_29dof_joint_pos,
    termination=termination.g1_29dof_termination,
    randomization=randomization.g1_29dof_randomization,
    command=command.g1_29dof_command,
    curriculum=curriculum.g1_29dof_curriculum,
    reward=reward.g1_29dof_loco,
)

g1_29dof_fpo_refdiag = ExperimentConfig(
    env_class="holosoma.envs.locomotion.locomotion_manager.LeggedRobotLocomotionManager",
    training=TrainingConfig(project="hv-g1-manager", name="g1_29dof_fpo_refdiag"),
    algo=replace(
        algo.fpo,
        config=replace(
            algo.fpo.config,
            num_learning_iterations=25000,
            use_symmetry=True,
            clip_param=0.01,
            lam=0.2,
            ratio_mode="legacy_avg",
            ratio_log_clip=1.0,
            num_mc_samples=16,
            mc_chunk_size=16,
            trust_region_mode="ppo_clip",
            cfm_reg_coef=0.0,
        ),
    ),
    simulator=simulator.isaacgym,
    robot=robot.g1_29dof,
    terrain=terrain.terrain_locomotion_mix,
    observation=observation.g1_29dof_loco_single_wolinvel,
    action=action.g1_29dof_joint_pos,
    termination=termination.g1_29dof_termination,
    randomization=randomization.g1_29dof_randomization,
    command=command.g1_29dof_command,
    curriculum=curriculum.g1_29dof_curriculum,
    reward=reward.g1_29dof_loco,
)

g1_29dof_fpo_data = ExperimentConfig(
    env_class="holosoma.envs.locomotion.locomotion_manager.LeggedRobotLocomotionManager",
    training=TrainingConfig(project="hv-g1-manager", name="g1_29dof_fpo_data"),
    algo=replace(
        algo.fpo,
        config=replace(
            algo.fpo.config,
            num_learning_iterations=25000,
            use_symmetry=True,
            clip_param=0.01,
            lam=0.5,
            flow_param_mode="data",
            cfm_reg_coef=0.05,
        ),
    ),
    simulator=simulator.isaacgym,
    robot=robot.g1_29dof,
    terrain=terrain.terrain_locomotion_mix,
    observation=observation.g1_29dof_loco_single_wolinvel,
    action=action.g1_29dof_joint_pos,
    termination=termination.g1_29dof_termination,
    randomization=randomization.g1_29dof_randomization,
    command=command.g1_29dof_command,
    curriculum=curriculum.g1_29dof_curriculum,
    reward=reward.g1_29dof_loco,
)

g1_29dof_fpo_pp_repro = ExperimentConfig(
    env_class="holosoma.envs.locomotion.locomotion_manager.LeggedRobotLocomotionManager",
    training=TrainingConfig(project="hv-g1-manager", name="g1_29dof_fpo_pp_repro"),
    algo=replace(
        algo.fpo,
        config=replace(
            algo.fpo.config,
            num_learning_iterations=25000,
            use_symmetry=True,
            num_learning_epochs=8,
            num_steps_per_env=24,
            num_mini_batches=4,
            lam=0.95,
            clip_param=0.05,
            cfm_reg_coef=0.0,
            num_flow_steps=64,
            num_mc_samples=16,
            ratio_mode="per_sample",
            ratio_log_clip=1.0,
            trust_region_mode="aspo",
            action_bound=1.5,
            action_bound_warmup_iters=0,
            max_grad_norm=0.5,
            flow_param_mode="velocity",
            cfm_loss_reduction="mean",
            cfm_loss_clip=10000.0,
            obs_normalization=True,
            divergence_guard_enabled=True,
            module_dict=FPOModuleDictConfig(
                actor=ModuleConfig(
                    type="MLP",
                    input_dim=["actor_obs"],
                    output_dim=["robot_action_dim"],
                    layer_config=LayerConfig(hidden_dims=[256, 256, 256], activation="ELU"),
                ),
                critic=ModuleConfig(
                    type="MLP",
                    input_dim=["critic_obs"],
                    output_dim=[1],
                    layer_config=LayerConfig(hidden_dims=[768, 768, 768], activation="ELU"),
                ),
            ),
        ),
    ),
    simulator=simulator.isaacgym,
    robot=robot.g1_29dof,
    terrain=terrain.terrain_locomotion_mix,
    observation=observation.g1_29dof_loco_single_wolinvel,
    action=action.g1_29dof_joint_pos,
    termination=termination.g1_29dof_termination,
    randomization=randomization.g1_29dof_randomization,
    command=command.g1_29dof_command,
    curriculum=curriculum.g1_29dof_curriculum,
    reward=reward.g1_29dof_loco,
)

g1_29dof_fpo_pp_warmstart_probe = ExperimentConfig(
    env_class="holosoma.envs.locomotion.locomotion_manager.LeggedRobotLocomotionManager",
    training=TrainingConfig(project="hv-g1-manager", name="g1_29dof_fpo_pp_warmstart_probe"),
    algo=replace(
        algo.fpo,
        config=replace(
            algo.fpo.config,
            num_learning_iterations=25000,
            use_symmetry=True,
            num_learning_epochs=8,
            num_steps_per_env=24,
            num_mini_batches=4,
            lam=0.95,
            clip_param=0.05,
            cfm_reg_coef=0.0,
            num_flow_steps=64,
            num_mc_samples=16,
            ratio_mode="per_sample",
            ratio_log_clip=1.0,
            trust_region_mode="aspo",
            action_bound=1.5,
            action_bound_warmup_iters=0,
            max_grad_norm=0.5,
            flow_param_mode="velocity",
            cfm_loss_reduction="mean",
            cfm_loss_clip=10000.0,
            obs_normalization=True,
            divergence_guard_enabled=True,
            # BC warm-start from PPO teacher
            warm_start_mode="bc_teacher",
            warm_start_checkpoint=None,  # Set via CLI: --algo.config.warm_start_checkpoint /path/to/ppo.pt
            warm_start_bc_steps=200,
            warm_start_load_critic=False,
            warm_start_teacher_module=ModuleConfig(
                type="MLP",
                input_dim=["actor_obs"],
                output_dim=["robot_action_dim"],
                layer_config=LayerConfig(hidden_dims=[512, 256, 128], activation="ELU"),
            ),
            module_dict=FPOModuleDictConfig(
                actor=ModuleConfig(
                    type="MLP",
                    input_dim=["actor_obs"],
                    output_dim=["robot_action_dim"],
                    layer_config=LayerConfig(hidden_dims=[256, 256, 256], activation="ELU"),
                ),
                critic=ModuleConfig(
                    type="MLP",
                    input_dim=["critic_obs"],
                    output_dim=[1],
                    layer_config=LayerConfig(hidden_dims=[768, 768, 768], activation="ELU"),
                ),
            ),
        ),
    ),
    simulator=simulator.isaacgym,
    robot=robot.g1_29dof,
    terrain=terrain.terrain_locomotion_mix,
    observation=observation.g1_29dof_loco_single_wolinvel,
    action=action.g1_29dof_joint_pos,
    termination=termination.g1_29dof_termination,
    randomization=randomization.g1_29dof_randomization,
    command=command.g1_29dof_command,
    curriculum=curriculum.g1_29dof_curriculum,
    reward=reward.g1_29dof_loco,
)

__all__ = [
    "g1_29dof",
    "g1_29dof_fast_sac",
    "g1_29dof_fpo",
    "g1_29dof_fpo_data",
    "g1_29dof_fpo_pp_repro",
    "g1_29dof_fpo_pp_warmstart_probe",
    "g1_29dof_fpo_refdiag",
]
